{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hEUkyTU-rXBE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "np.random.seed(0)\n",
        "import colorsys\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKBqyfDqQPMS",
        "outputId": "c992a6ea-02f9-49e0-ae15-59906deee515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mounting google drive for running in google colab, comment if running locally and update directories in the next cell\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pkZt5bIQb4q",
        "outputId": "538c6088-b575-4ac1-fee7-af89f9bf0b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found - 1744 images & 1744 labels\n"
          ]
        }
      ],
      "source": [
        "# change if running locally\n",
        "image_files = os.listdir('./drive/MyDrive/QC/rgb')\n",
        "csv_files = os.listdir('./drive/MyDrive/QC/csv')\n",
        "\n",
        "print(f\"Found - {len(image_files)} images & {len(csv_files)} labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BGZqIqUgwSzA"
      },
      "outputs": [],
      "source": [
        "a = np.random.choice(np.arange(0,len(csv_files)),size=250, replace = False).tolist()\n",
        "with open(\"./chosen_indices.json\",\"w\") as f:\n",
        "  json.dump({\n",
        "      \"indices\" : a,\n",
        "  }, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GkmZPHByuouf"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(csv_files, slice_ = 36, destination = \"./processed_data\", convert_to_gray_scale = False):\n",
        "\n",
        "  print(\"Reading nuclei labels - \")\n",
        "  data = pd.DataFrame()\n",
        "\n",
        "  for f in tqdm(csv_files):\n",
        "    temp = pd.read_csv('./drive/MyDrive/QC/csv/' + f, engine=  \"python\")\n",
        "    temp[\"slide\"] = f.split(\".\")[0]\n",
        "    temp.drop(columns= [\"Unnamed: 0\"], inplace=True)\n",
        "    data = pd.concat([data, temp], axis=0)  \n",
        "  \n",
        "  print(\"Read nuclei labels, removing files with smaller than required slices\")\n",
        "  data[\"xrange\"] = data[\"xmax\"] - data[\"xmin\"]\n",
        "  data[\"yrange\"] = data[\"ymax\"] - data[\"ymin\"]  \n",
        "  data[\"aspect_ratio\"] = data[\"xrange\"]/data[\"yrange\"]\n",
        "\n",
        "  print(\"Removed smaller slices, processing labels\")\n",
        "\n",
        "  data = data[data[\"raw_classification\"] != \"unlabeled\"]\n",
        "  print(data[\"raw_classification\"].value_counts())\n",
        "  data[\"label\"] = data[\"raw_classification\"].apply(lambda x: x if x==\"tumor\" else \"not-tumor\")\n",
        "\n",
        "  print(\"Processed labels - \", data[\"label\"].value_counts())\n",
        "  print(\"Reading files - \")\n",
        "  if not os.path.isdir(destination):\n",
        "    os.mkdir(destination)\n",
        "  else:\n",
        "    print(\"Overwriting existing data\")\n",
        "  final_labelled_data = {\n",
        "      \"filename\" : [],\n",
        "      \"label\" : []\n",
        "  }\n",
        "  i = 0\n",
        "  for f in tqdm(data[\"slide\"].unique()):\n",
        "    subset = data[data[\"slide\"] == f]\n",
        "    filename = './drive/MyDrive/QC/rgb/' + f + \".png\"\n",
        "    image = cv2.imread(filename)\n",
        "\n",
        "    for row in subset.iterrows():\n",
        "      row = row[1]\n",
        "      image_ = image[row[\"ymin\"]:row[\"ymax\"]+1, row[\"xmin\"]:row[\"xmax\"]+1]\n",
        "      x = int(row[\"xmin\"] + row[\"xrange\"]/2)\n",
        "      y = int(row[\"ymin\"] + row[\"yrange\"]/2)\n",
        "      if x<18 or y<18 or x+18>image.shape[0] or y+18>image.shape[1]:\n",
        "        continue\n",
        "      image_ = image[x-18:x+18, y-18:y+18]\n",
        "      image_ = cv2.resize(image_, (slice_, slice_), interpolation = cv2.INTER_AREA)\n",
        "      if convert_to_gray_scale:\n",
        "        assert slice_ == 28\n",
        "        image_ = cv2.cvtColor(image_, cv2.COLOR_RGB2GRAY)        \n",
        "        image_ = cv2.resize(image_, (slice_, slice_), interpolation = cv2.INTER_AREA)\n",
        "      np.save(f\"{destination}/{i}\", np.moveaxis(image_,-1,0))\n",
        "      flipped_image_ = cv2.flip(image_, 1)\n",
        "      np.save(f\"{destination}/{i}_flipped\", np.moveaxis(flipped_image_,-1,0))\n",
        "      final_labelled_data[\"filename\"].append(i)\n",
        "      final_labelled_data[\"label\"].append(row[\"label\"])\n",
        "\n",
        "      rot_image_ = cv2.rotate(image_, cv2.ROTATE_90_CLOCKWISE)\n",
        "      np.save(f\"{destination}/{i}_rot90\", np.moveaxis(rot_image_,-1,0))\n",
        "\n",
        "      rot_image_ = cv2.rotate(image_, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "      np.save(f\"{destination}/{i}_rot270\", np.moveaxis(rot_image_,-1,0))\n",
        "\n",
        "      rot_image_ = cv2.rotate(image_, cv2.ROTATE_180)\n",
        "      np.save(f\"{destination}/{i}_rot180\", np.moveaxis(rot_image_,-1,0))\n",
        "\n",
        "      rot_image_ = cv2.rotate(flipped_image_, cv2.ROTATE_90_CLOCKWISE)\n",
        "      np.save(f\"{destination}/{i}_flipped_rot90\", np.moveaxis(rot_image_,-1,0))\n",
        "\n",
        "      rot_image_ = cv2.rotate(flipped_image_, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "      np.save(f\"{destination}/{i}_flipped_rot270\", np.moveaxis(rot_image_,-1,0))\n",
        "\n",
        "      rot_image_ = cv2.rotate(flipped_image_, cv2.ROTATE_180)\n",
        "      np.save(f\"{destination}/{i}_flipped_rot180\", np.moveaxis(rot_image_,-1,0))\n",
        "      i+=1\n",
        "  \n",
        "  final_labelled_data = pd.DataFrame(final_labelled_data)\n",
        "  print(final_labelled_data[\"label\"].value_counts())\n",
        "  final_labelled_data.to_csv(f\"{destination}/final_labelled_data.csv\", index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9koDb14Kvosf",
        "outputId": "9e8495fd-a58e-4959-b1df-d92f1ff1e00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading nuclei labels - \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:36<00:00,  6.86it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read nuclei labels, removing files with smaller than required slices\n",
            "Removed smaller slices, processing labels\n",
            "tumor                   2725\n",
            "lymphocyte              1959\n",
            "fibroblast              1215\n",
            "plasma_cell              855\n",
            "macrophage               246\n",
            "ductal_epithelium        124\n",
            "vascular_endothelium      82\n",
            "apoptotic_body            58\n",
            "mitotic_figure            29\n",
            "myoepithelium             17\n",
            "neutrophil                 1\n",
            "eosinophil                 1\n",
            "Name: raw_classification, dtype: int64\n",
            "Processed labels -  not-tumor    4587\n",
            "tumor        2725\n",
            "Name: label, dtype: int64\n",
            "Reading files - \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [03:54<00:00,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not-tumor    4112\n",
            "tumor        2497\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "csv_files = np.asarray(csv_files)\n",
        "prepare_dataset(csv_files[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjFnSoiTL8GR",
        "outputId": "f040ab48-d7fc-4447-bdc6-307cb304eb61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading nuclei labels - \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:01<00:00, 195.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read nuclei labels, removing files with smaller than required slices\n",
            "Removed smaller slices, processing labels\n",
            "tumor                   2725\n",
            "lymphocyte              1959\n",
            "fibroblast              1215\n",
            "plasma_cell              855\n",
            "macrophage               246\n",
            "ductal_epithelium        124\n",
            "vascular_endothelium      82\n",
            "apoptotic_body            58\n",
            "mitotic_figure            29\n",
            "myoepithelium             17\n",
            "neutrophil                 1\n",
            "eosinophil                 1\n",
            "Name: raw_classification, dtype: int64\n",
            "Processed labels -  not-tumor    4587\n",
            "tumor        2725\n",
            "Name: label, dtype: int64\n",
            "Reading files - \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:11<00:00, 22.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not-tumor    4112\n",
            "tumor        2497\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "prepare_dataset(csv_files[a], slice_ = 28, destination=\"./processed_grayscale_data\", convert_to_gray_scale = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SsHQIbqQ2jRU"
      },
      "outputs": [],
      "source": [
        "all_data = pd.read_csv(\"./processed_data/final_labelled_data.csv\")\n",
        "total = np.arange(0,np.max(all_data[\"filename\"].values))\n",
        "x_train_idx, x_test_idx = train_test_split(total, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eZAR40yH1LKq"
      },
      "outputs": [],
      "source": [
        "def fetch_data(filename = \"./processed_data/final_labelled_data.csv\", destination = \"./processed_data/\"):\n",
        "  global x_train_idx, x_test_idx\n",
        "  all_data = pd.read_csv(filename)\n",
        "  total = np.arange(0,np.max(all_data[\"filename\"].values))\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  x_test = []\n",
        "  y_test = []\n",
        "  train_data = all_data[all_data[\"filename\"].isin(x_train_idx)]\n",
        "  print(train_data.shape)\n",
        "  for i, row in train_data.iterrows():\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \".npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_flipped.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_rot90.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)    \n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_rot180.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_rot270.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_flipped_rot90.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)    \n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_flipped_rot180.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \"_flipped_rot270.npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_train.append(data/255.)\n",
        "    y_train.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "  test_data = all_data[all_data[\"filename\"].isin(x_test_idx)]\n",
        "  for i, row in test_data.iterrows():\n",
        "    data = np.load(destination + str(row[\"filename\"]) + \".npy\")\n",
        "    # data = cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n",
        "    x_test.append(data/255.)\n",
        "    y_test.append(1 if row[\"label\"]==\"tumor\" else 0)\n",
        "  return np.asarray(x_train), np.asarray(y_train), np.asarray(x_test), np.asarray(y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qky9wK23xCqE"
      },
      "source": [
        "#LeNet and Color LeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Im5p3sW9vtwz"
      },
      "outputs": [],
      "source": [
        "class color_lenet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(color_lenet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=50, kernel_size = 5)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(in_features=3600, out_features=500)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(in_features=500, out_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GNS0Zh2VFh-k"
      },
      "outputs": [],
      "source": [
        "class lenet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(lenet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size = 5)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(in_features=800, out_features=500)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(in_features=500, out_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JugvdgGRvySY",
        "outputId": "1ee3431c-7c1b-414f-c0ed-75568ad0d9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5286, 2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(42288, 3, 36, 36)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train, y_train, x_test, y_test = fetch_data()\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nXIrmzUIxe9g"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.from_numpy(x_train)\n",
        "tensor_y = torch.from_numpy(y_train)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YprIUKGF50LX",
        "outputId": "fcabe7de-14d7-496b-b6e6-0d49f5b81ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 5, loss = 0.6669192907940061\n",
            "Epoch - 10, loss = 0.6533789293320621\n",
            "Epoch - 15, loss = 0.627954635279339\n",
            "Epoch - 20, loss = 0.616277405262397\n",
            "Epoch - 25, loss = 0.6110172814338394\n",
            "Epoch - 30, loss = 0.6021737438596999\n",
            "Epoch - 35, loss = 0.602004487667678\n",
            "Epoch - 40, loss = 0.5957117552282201\n",
            "Epoch - 45, loss = 0.5906993420989434\n",
            "Epoch - 50, loss = 0.5760026194886213\n",
            "Epoch - 55, loss = 0.5687105588263586\n",
            "Epoch - 60, loss = 0.5617053931107686\n",
            "Epoch - 65, loss = 0.5473009549111398\n",
            "Epoch - 70, loss = 0.5289199350445359\n",
            "Epoch - 75, loss = 0.4806801386490329\n",
            "Epoch - 80, loss = 0.46958843397988226\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 82\n",
        "losses = []\n",
        "\n",
        "model = color_lenet()\n",
        "model.train()\n",
        "model.cuda()\n",
        "loss_fn = nn.BCELoss()\n",
        "sgd = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 5e-4)\n",
        "sgd2 = optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.9, weight_decay = 5e-4)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  avg_loss = 0\n",
        "  for i , data in enumerate(train_dataloader):\n",
        "    x,y = data\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    if epoch < 70:\n",
        "      sgd.zero_grad()\n",
        "    else:\n",
        "      sgd2.zero_grad()\n",
        "    pred = model(x.float())\n",
        "    loss = loss_fn(pred, y.float().view(-1,1))\n",
        "    loss.backward()\n",
        "\n",
        "    if epoch < 70:\n",
        "      sgd.step()\n",
        "    else:\n",
        "      sgd2.step()\n",
        "    avg_loss += loss.item()\n",
        "    losses.append(avg_loss/len(train_dataloader))\n",
        "  if (epoch+1)%5 == 0:\n",
        "    print(f\"Epoch - {epoch+1}, loss = {avg_loss/len(train_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qUWPt2Zl7gEy"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.from_numpy(x_test)\n",
        "tensor_y = torch.from_numpy(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 64)\n",
        "\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(train_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  train_predictions.extend(pred)\n",
        "  train_labels.extend(y.numpy().tolist())\n",
        "\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(test_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  test_predictions.extend(pred)\n",
        "  test_labels.extend(y.numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EiIiU4LS-R8G"
      },
      "outputs": [],
      "source": [
        "train_predictions = [1 if x[0] > 0.5 else 0 for x in train_predictions]\n",
        "test_predictions = [1 if x[0] > 0.5 else 0 for x in test_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCFBuf3T-Ytb",
        "outputId": "c3fe6280-5917-4935-eefd-18fa301b1526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n",
            "Accuracy = 0.7857311766931517\n",
            "Precision = 0.6753246753246753, Recall = 0.7369353410097431, F-score = 0.7047861075815333\n",
            "Accuracy = 0.7307110438729199\n",
            "Precision = 0.5959595959595959, Recall = 0.6541019955654102, F-score = 0.623678646934461\n",
            "G-measure = 0.7098636202715726\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training\\nAccuracy = {accuracy_score(train_predictions, train_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(train_predictions, train_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"Accuracy = {accuracy_score(test_predictions, test_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(test_predictions, test_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"G-measure = {geometric_mean_score(test_predictions, test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxs8UaFkMbNt",
        "outputId": "905ae603-394c-4323-cadb-b9bf182a152c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5286, 2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(42288, 28, 28)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train, y_train, x_test, y_test = fetch_data(destination = \"./processed_grayscale_data/\")\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkxBH2aLMtdQ"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.from_numpy(x_train.reshape(x_train.shape[0],1,28,28))\n",
        "tensor_y = torch.from_numpy(y_train)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxa6_HNaM3Io",
        "outputId": "c7c1acb3-2f9e-46f8-aaa9-0fb564c3833a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 5, loss = 0.6596542340555368\n",
            "Epoch - 10, loss = 0.6327451292401441\n",
            "Epoch - 15, loss = 0.625298880802893\n",
            "Epoch - 20, loss = 0.6215092152950273\n",
            "Epoch - 25, loss = 0.6184093433385786\n",
            "Epoch - 30, loss = 0.6157982582015051\n",
            "Epoch - 35, loss = 0.6135936388668412\n",
            "Epoch - 40, loss = 0.6115369703190939\n",
            "Epoch - 45, loss = 0.6095516331150068\n",
            "Epoch - 50, loss = 0.6076937319940511\n",
            "Epoch - 55, loss = 0.6059145781418672\n",
            "Epoch - 60, loss = 0.6041368505897764\n",
            "Epoch - 65, loss = 0.6024764412380023\n",
            "Epoch - 70, loss = 0.6006611720644749\n",
            "Epoch - 75, loss = 0.598919209834673\n",
            "Epoch - 80, loss = 0.5986009679154215\n",
            "Epoch - 85, loss = 0.5949911940758329\n",
            "Epoch - 90, loss = 0.59286001466395\n",
            "Epoch - 95, loss = 0.5903351053478599\n",
            "Epoch - 100, loss = 0.5877407026137599\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 100\n",
        "losses = []\n",
        "\n",
        "model = lenet()\n",
        "model.cuda()\n",
        "loss_fn = nn.BCELoss()\n",
        "sgd = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 5e-4)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  avg_loss = 0\n",
        "  for i , data in enumerate(train_dataloader):\n",
        "    x,y = data\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    sgd.zero_grad()\n",
        "    pred = model(x.float())\n",
        "    loss = loss_fn(pred, y.float().view(-1,1))\n",
        "    loss.backward()\n",
        "    sgd.step()\n",
        "\n",
        "    avg_loss += loss.item()\n",
        "    losses.append(avg_loss/len(train_dataloader))\n",
        "  if (epoch+1)%5 == 0:\n",
        "    print(f\"Epoch - {epoch+1}, loss = {avg_loss/len(train_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOxkU7fcM82E"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.from_numpy(x_test.reshape(x_test.shape[0],1,28,28))\n",
        "tensor_y = torch.from_numpy(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 64)\n",
        "\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(train_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  train_predictions.extend(pred)\n",
        "  train_labels.extend(y.numpy().tolist())\n",
        "\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(test_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  test_predictions.extend(pred)\n",
        "  test_labels.extend(y.numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWeGf7ceNDkm"
      },
      "outputs": [],
      "source": [
        "train_predictions = [1 if x[0] > 0.5 else 0 for x in train_predictions]\n",
        "test_predictions = [1 if x[0] > 0.5 else 0 for x in test_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPVadLnqNGNf",
        "outputId": "375c4d6a-f0d3-4aae-da6b-c22d6b3424a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n",
            "Accuracy = 0.6815645100264851\n",
            "Precision = 0.3778096903096903, Recall = 0.6334798994974874, F-score = 0.4733260325406759\n",
            "Accuracy = 0.6732223903177005\n",
            "Precision = 0.36767676767676766, Recall = 0.6046511627906976, F-score = 0.457286432160804\n",
            "G-measure = 0.6475244981965965\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training\\nAccuracy = {accuracy_score(train_predictions, train_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(train_predictions, train_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"Accuracy = {accuracy_score(test_predictions, test_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(test_predictions, test_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"G-measure = {geometric_mean_score(test_predictions, test_labels)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmLlcefCSTa"
      },
      "source": [
        "#Color-Encoder-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "7BDNovTS-ZE7"
      },
      "outputs": [],
      "source": [
        "class encoder_net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(encoder_net, self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=36*36*3, out_features=6000)\n",
        "    self.fc2 = nn.Linear(in_features=6000, out_features=12000)\n",
        "    self.fc3 = nn.Linear(in_features=12000, out_features=3000)\n",
        "    self.fc4 = nn.Linear(in_features = 3000, out_features = 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mYVIZIzEPCZx"
      },
      "outputs": [],
      "source": [
        "class encoder_net_gray(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(encoder_net_gray, self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=28*28*1, out_features=1000)\n",
        "    self.fc2 = nn.Linear(in_features=1000, out_features=500)\n",
        "    self.fc3 = nn.Linear(in_features=500, out_features=250)\n",
        "    self.fc4 = nn.Linear(in_features = 250, out_features = 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    # x = self.sigmoid(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc4(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmefLRJyDUVZ",
        "outputId": "9f1ee32d-5a52-4b00-82ce-7ef28e006e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5286, 2)\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train, x_test, y_test = fetch_data()\n",
        "x_train.shape\n",
        "tensor_x = torch.from_numpy(x_train.reshape(x_train.shape[0], -1))\n",
        "tensor_y = torch.from_numpy(y_train)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QrrPxsGDe2r",
        "outputId": "4a41b420-7398-4218-8990-dd48c2a49a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch - 1, loss = 0.6784283227949257\n",
            "Epoch - 2, loss = 0.6639528707208404\n",
            "Epoch - 3, loss = 0.6594203793140779\n",
            "Epoch - 4, loss = 0.6563060549368341\n",
            "Epoch - 5, loss = 0.655857325318348\n",
            "Epoch - 6, loss = 0.6550078979098654\n",
            "Epoch - 7, loss = 0.6551069944917437\n",
            "Epoch - 8, loss = 0.6550899577068995\n",
            "Epoch - 9, loss = 0.6537540290190513\n",
            "Epoch - 10, loss = 0.6511558814041586\n",
            "Epoch - 11, loss = 0.6507203119346895\n",
            "Epoch - 12, loss = 0.6495675200439361\n",
            "Epoch - 13, loss = 0.6484157092061388\n",
            "Epoch - 14, loss = 0.6473390816385487\n",
            "Epoch - 15, loss = 0.6437328070940742\n",
            "Epoch - 16, loss = 0.6462241131139089\n",
            "Epoch - 17, loss = 0.6455322003149124\n",
            "Epoch - 18, loss = 0.6443813374961715\n",
            "Epoch - 19, loss = 0.6422863617000809\n",
            "Epoch - 20, loss = 0.64163013772074\n",
            "Epoch - 21, loss = 0.6374567997563316\n",
            "Epoch - 22, loss = 0.6400381200105311\n",
            "Epoch - 23, loss = 0.6320033503404583\n",
            "Epoch - 24, loss = 0.6322170329919781\n",
            "Epoch - 25, loss = 0.6430782498904022\n",
            "Epoch - 26, loss = 0.6296628999063768\n",
            "Epoch - 27, loss = 0.6302390208086336\n",
            "Epoch - 28, loss = 0.6307346529091697\n",
            "Epoch - 29, loss = 0.6270567320556526\n",
            "Epoch - 30, loss = 0.6246180511921285\n",
            "Epoch - 31, loss = 0.6187344897224243\n",
            "Epoch - 32, loss = 0.6192790231701121\n",
            "Epoch - 33, loss = 0.6248430451714849\n",
            "Epoch - 34, loss = 0.6145132495516754\n",
            "Epoch - 35, loss = 0.6143516491963921\n",
            "Epoch - 36, loss = 0.6143240182245352\n",
            "Epoch - 37, loss = 0.6148942324770502\n",
            "Epoch - 38, loss = 0.6173656744411192\n",
            "Epoch - 39, loss = 0.6212354859853365\n",
            "Epoch - 40, loss = 0.6150530050676989\n",
            "Epoch - 41, loss = 0.611611040331513\n",
            "Epoch - 42, loss = 0.6096382581864495\n",
            "Epoch - 43, loss = 0.6086370240313461\n",
            "Epoch - 44, loss = 0.6057774222579347\n",
            "Epoch - 45, loss = 0.6034131188349552\n",
            "Epoch - 46, loss = 0.6035693598439894\n",
            "Epoch - 47, loss = 0.6007805292505816\n",
            "Epoch - 48, loss = 0.6016669227595789\n",
            "Epoch - 49, loss = 0.5973726040627583\n",
            "Epoch - 50, loss = 0.595104823450008\n",
            "Epoch - 51, loss = 0.595024104039353\n",
            "Epoch - 52, loss = 0.5921876555286258\n",
            "Epoch - 53, loss = 0.5933928107281765\n",
            "Epoch - 54, loss = 0.5939402087445719\n",
            "Epoch - 55, loss = 0.594234677861972\n",
            "Epoch - 56, loss = 0.5918716586318361\n",
            "Epoch - 57, loss = 0.5883285420307194\n",
            "Epoch - 58, loss = 0.58881636592279\n",
            "Epoch - 59, loss = 0.5933212715100094\n",
            "Epoch - 60, loss = 0.5886842835559902\n",
            "Epoch - 61, loss = 0.5900076909596661\n",
            "Epoch - 62, loss = 0.5844941102417118\n",
            "Epoch - 63, loss = 0.5888512972966734\n",
            "Epoch - 64, loss = 0.5857035305844732\n",
            "Epoch - 65, loss = 0.5843283841947475\n",
            "Epoch - 66, loss = 0.5838431068303356\n",
            "Epoch - 67, loss = 0.5808745455670069\n",
            "Epoch - 68, loss = 0.5790924060506275\n",
            "Epoch - 69, loss = 0.5784772527684648\n",
            "Epoch - 70, loss = 0.5857123802584338\n",
            "Epoch - 71, loss = 0.5805152257881969\n",
            "Epoch - 72, loss = 0.583184126988951\n",
            "Epoch - 73, loss = 0.5786731653364308\n",
            "Epoch - 74, loss = 0.5754726988274649\n",
            "Epoch - 75, loss = 0.5759353526834264\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 75\n",
        "losses = []\n",
        "\n",
        "model = encoder_net()\n",
        "model.cuda()\n",
        "loss_fn = nn.BCELoss()\n",
        "sgd = optim.SGD(model.parameters(), lr = 0.005, momentum = 0.9, weight_decay = 5e-4)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  avg_loss = 0\n",
        "  for i , data in enumerate(train_dataloader):\n",
        "    x,y = data\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    sgd.zero_grad()\n",
        "    pred = model(x.float())\n",
        "    loss = loss_fn(pred, y.float().view(-1,1))\n",
        "    loss.backward()\n",
        "    sgd.step()\n",
        "\n",
        "    avg_loss += loss.item()\n",
        "    losses.append(avg_loss/len(train_dataloader))\n",
        "  # if (epoch+1)%5 == 0:\n",
        "  print(f\"Epoch - {epoch+1}, loss = {avg_loss/len(train_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "pOOc4rwKDicM"
      },
      "outputs": [],
      "source": [
        "tensor_x = torch.from_numpy(x_test.reshape(x_test.shape[0], -1))\n",
        "tensor_y = torch.from_numpy(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 64)\n",
        "\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(train_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  train_predictions.extend(pred)\n",
        "  train_labels.extend(y.numpy().tolist())\n",
        "\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "model.eval()\n",
        "for i , data in enumerate(test_dataloader):\n",
        "  x,y = data\n",
        "  x = x.cuda()\n",
        "  pred = model(x.float()).cpu().detach().numpy().tolist()\n",
        "  test_predictions.extend(pred)\n",
        "  test_labels.extend(y.numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "R_gZ7eiTEraJ"
      },
      "outputs": [],
      "source": [
        "train_predictions = [1 if x[0] > 0.5 else 0 for x in train_predictions]\n",
        "test_predictions = [1 if x[0] > 0.5 else 0 for x in test_predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B4q1WrUEysF",
        "outputId": "527c3d6a-6343-45a5-bbfa-42127dd4dc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n",
            "Accuracy = 0.693837495270526\n",
            "Precision = 0.3793081918081918, Recall = 0.6689791873141725, F-score = 0.4841216081603379\n",
            "Accuracy = 0.7299546142208775\n",
            "Precision = 0.4404040404040404, Recall = 0.7315436241610739, F-score = 0.5498108448928121\n",
            "G-measure = 0.7305171857259346\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training\\nAccuracy = {accuracy_score(train_predictions, train_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(train_predictions, train_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"Accuracy = {accuracy_score(test_predictions, test_labels)}\")\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(test_predictions, test_labels, average=\"binary\")\n",
        "print(f\"Precision = {precision}, Recall = {recall}, F-score = {fscore}\")\n",
        "print(f\"G-measure = {geometric_mean_score(test_predictions, test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSpoI4NjRRSl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
